{"steps":[{"type":"step","value":[{"type":"h2","value":[{"type":"text","value":"1. Introduction"}]},{"type":"p","value":[{"type":"text","value":"Imagine you’re a cattologist, trying to identify <b>different groups</b> of species of cats (rocks / minerals). You go out into the field to collect measurements of <i>chonkiness</i> and <i>fur color</i>—key features which you believe are important in their differentiation."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"So you have at hand measurements of 100 cats you’ve encountered."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"Visualizing them as a scatter plot, it turns out that you can conveniently identify <b>distinct clusters</b>—cats with similar measurements must be from the same species."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"However, this is admittedly a contrived example—real-life datasets don’t necessarily look as elegant, and we are typically interested in more than two features. This makes it non-trivial to visualize, and picking out clusters through visual observation alone is near futile."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"Hence, given a dataset of observations with features we care about, is there a way to <i>automatically</i> demarcate clusters from them? In machine learning, we call such methods <b>clustering algorithms</b>, and they are <i>unsupervised</i>—we don’t have any information about the ground-truth clusters, and are tasked to infer structure just from the features we have at hand."}]}]},{"type":"step","value":[{"type":"h2","value":[{"type":"text","value":"2. K-Means"}]},{"type":"p","value":[{"type":"text","value":"How can we determine if any two observations belong to the same cluster? Intuitively, we want to assign observations that are similar to the same cluster, and at the same time placing dissimilar observations in different clusters."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"We’ll first need to define a <b>notion of similarity</b> between two observations. While there are many ways to do so, a natural choice would the straight line distance—or better known as the Euclidean distance—between them."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"The k-means algorithm requires that the users specify how many clusters they expect beforehand. Then, instead of explicitly learning the cluster assignments for each data point, it does so implicitly by learning what the exemplary, or <b>prototypical</b> example from each cluster looks like."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"To determine which cluster a data point belongs to, we just have to check which prototype it is the closest to."}]}]},{"type":"step","value":[{"type":"h3","value":[{"type":"text","value":"2.1 Objective"}]},{"type":"p","value":[{"type":"text","value":"Like most machine learning models, in k-means the learning process is guided by a <b>loss function</b>, which is essentially a score that indicates how bad of a fit the current prototypes are. The algorithm will strive to find a set of prototypes that minimizes this loss function."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"How can we measure the quality of the clustering in a single number? Intuitively, we want to make sure that every individual cluster is as tightly packed as possible; or that each cluster prototype is as close as possible to the data points it is assigned with."}]},{"type":"p","value":[{"type":"text","value":"More formally, we can define the loss function as: … This value is also known as the <b>inertia</b>."}]}]},{"type":"step","value":[{"type":"h3","value":[{"type":"text","value":"2.2 Learning Algorithm"}]},{"type":"p","value":[{"type":"text","value":"Here comes the fun part: how does k-means perform this automatically? The algorithm is as follows:"}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"It first makes a guess for where the prototypes are."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"As mentioned, it can perform an initial clustering assignment by matching each data point to the prototype it is the closest to. This ensures that the present inertia is the lowest, given the current prototypes."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"To improve upon this initial estimate of the prototypes, it updates them to be the <b>mean or center</b> of the data points newly assigned to them, making them more prototypical. (As you might’ve guessed, this is where the name k-means comes from, and hence why the prototypes are also referred to as centroids).  This too minimizes the inertia, given the current cluster assignments."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"The algorithm then repeatedly alternates between these two minimization steps. In fact, it can be shown mathematically that each step will always decrease the inertia. When the prototypes no longer update by a significant amount, we deem the resulting cluster assignments as our final solution."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"Sample paragraph 1"}]},{"type":"p","value":[{"type":"text","value":"Sample paragraph 2"},{"type":"b","value":"Bolded word"}]}]}]}