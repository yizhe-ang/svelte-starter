{"steps":[{"type":"step","value":[{"type":"SectionTitle","props":{"tag":"h2","index":"1.","title":"Introduction"}},{"type":"p","value":[{"type":"text","value":"Imagine you’re a felineologist, trying to identify <b>different groups</b> of species of cats. You go out into the field to collect measurements of <i>chonkiness</i> and <i>fur color</i>—key features which you believe are important in their differentiation."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"So you have at hand measurements of 100 cats you’ve encountered."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"Visualizing them as a scatter plot, it turns out that you can conveniently identify <b>distinct clusters</b>—cats with similar measurements must be from the same species."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"However, this is admittedly a contrived example—real-life datasets don’t necessarily look as elegant, and we are typically interested in more than two features. This makes it non-trivial to visualize, and picking out clusters through visual observation alone is near futile."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"Hence, given a dataset of observations with features we care about, is there a way to <i>automatically</i> demarcate clusters from them? In machine learning, we call such methods <b>clustering algorithms</b>, and they are <i>unsupervised</i>—we don’t have any information about the ground-truth clusters, and are tasked to infer structure just from the features we have at hand."}]}]},{"type":"step","value":[{"type":"SectionTitle","props":{"tag":"h2","index":"2.","title":"The Algorithm"}},{"type":"p","value":[{"type":"text","value":"How can we determine if any two observations belong to the same cluster? Intuitively, we want to assign observations that are similar to the same cluster, and at the same time placing dissimilar observations in different clusters."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"We’ll first need to define a <b>notion of similarity</b> between two observations. While there are many ways to do so, a natural choice would the straight line distance—or better known as the <i>Euclidean distance</i>—between them."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"The k-means algorithm requires that the users specify how many clusters they expect beforehand (in this simple example, we expect that k = 3). Then, instead of explicitly learning the cluster assignments for each data point, it does so implicitly by learning what the exemplary, or <b>prototypical</b> example from each cluster looks like."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"To determine which cluster a data point belongs to, we just have to"},{"type":"PointerSvg","value":""},{"type":"text","value":"check which prototype it is the closest to."}]}]},{"type":"step","value":[{"type":"SectionTitle","props":{"tag":"h3","index":"2.1","title":"Learning Objective"}},{"type":"p","value":[{"type":"text","value":"Like most machine learning models, in k-means the learning process is guided by a <b>loss function</b>, which is essentially a score that indicates how bad of a fit the current prototypes are. The algorithm will strive to find a set of prototypes that minimizes this loss function."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"How can we measure the quality of clustering in a single number? Intuitively, we want to make sure that every individual cluster is as <i>tightly packed</i> as possible; or that each cluster prototype is as close as possible to the data points it is assigned with."}]},{"type":"p","value":[{"type":"text","value":"More formally, we can define the loss function as the sum of all Euclidean distances from each data point to its assigned prototype. This measures how internally coherent the clusters are, and is also known as the <b>inertia</b>."}]},{"type":"p","value":[{"type":"PointerSvg","value":""},{"type":"text","value":"Try adjusting and “learning” the prototypes for yourself; how do the cluster assignments, and the value of the inertia change? At which positions do the prototypes appear optimal or non-optimal?"}]}]},{"type":"step","value":[{"type":"SectionTitle","props":{"tag":"h3","index":"2.2","title":"Learning Algorithm"}},{"type":"p","value":[{"type":"text","value":"Here comes the fun part: how does k-means perform this automatically? The algorithm is as follows:"}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"It first makes a random guess for where the prototypes are."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"As mentioned, it can perform an initial clustering assignment by matching each data point to the prototype it is the closest to. This ensures that the value of the inertia is the lowest, given the <i>current prototypes</i>."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"To improve upon this initial estimate of the prototypes, it updates them to be the <b>mean or center</b> of the data points newly assigned to them, making them more prototypical (as you might’ve guessed, this is where the name k-<i>means</i> comes from, and hence why the prototypes are also referred to as centroids).  This too minimizes the inertia, given the <i>current cluster assignments</i>."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"The algorithm then repeatedly alternates between these two minimization steps; reassigning the data points to clusters, and adjusting the prototypes. In fact, it can be shown mathematically that each step will always decrease the inertia. When the prototypes no longer update by a significant amount, we deem the resulting cluster assignments as our final solution."}]},{"type":"p","value":[{"type":"text","value":"The final solution forms what is called a <a href=https://en.wikipedia.org/wiki/Voronoi_diagram>voronoi tessellation</a>; the data space is colored according to which cluster a data point would belong to if it was in that location."}]}]},{"type":"step","value":[{"type":"p","value":[{"type":"text","value":"Let’s look at the algorithm again. Step through it at your own pace!"}]},{"type":"KMeansStepper"}]},{"type":"step","value":[{"type":"SectionTitle","props":{"tag":"h2","index":"3.","title":"Properties"}},{"type":"p","value":[{"type":"text","value":"Every algorithm or model comes with its own set of assumptions about the task at hand. Let’s explore some properties of k-means by poking and prodding around its parameters and the dataset."}]}]},{"type":"step","value":[{"type":"SectionTitle","props":{"tag":"h3","index":"3.1","title":"Type of Dataset"}},{"type":"p","value":[{"type":"text","value":"K-means finds roughly circular clusters and assumes that they have the same variance. You can imagine a circle emanating at the same rate from each prototype."}]},{"type":"p","value":[{"type":"text","value":"But surely datasets and clusters come in all kinds of shapes and sizes. On which datasets does k-means not produce the expected clustering?"}]},{"type":"PlaygroundControls"},{"type":"p","value":[{"type":"text","value":"Due to the assumptions of k-means, it appears that k-means does react poorly to elongated clusters and irregular shapes."}]}]},{"type":"step","value":[{"type":"SectionTitle","props":{"tag":"h3","index":"3.2","title":"Non-deterministic Results"}},{"type":"p","value":[{"type":"text","value":"You may have noticed that a different initial guess of the prototypes sometimes leads to a different clustering result. While it is a fact that the inertia or loss function constantly decreases during the course of the algorithm, it may not eventually find <i>the</i> optimum solution, but rather the best it can do during the current run given the initial conditions, producing a <b>local minimum</b>."}]},{"type":"p","value":[{"type":"text","value":"Verify this for yourself by rerunning the algorithm multiple times and observing the results!"}]},{"type":"PlaygroundControls"},{"type":"p","value":[{"type":"text","value":"To generate a reliable result, <a href=https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html>scikit-learn’s</a> implementation by default uses an initialization scheme called <a href=https://en.wikipedia.org/wiki/K-means%2B%2B>k-means++</a> which initializes prototypes that are far away from one another, and returns the result with the lowest inertia over 10 different runs."}]}]},{"type":"step","value":[{"type":"SectionTitle","props":{"tag":"h3","index":"3.3","title":"Choice of K"}},{"type":"p","value":[{"type":"text","value":"A key drawback of k-means is having to specify the number of clusters beforehand, which can be problematic if you lack the domain knowledge or information about your dataset."}]},{"type":"p","value":[{"type":"text","value":"Tinker with the choice of k, specifying more or less clusters than what the dataset suggests. Do the resulting clusters still make sense?"}]},{"type":"PlaygroundControls"}]},{"type":"step","value":[{"type":"SectionTitle","props":{"tag":"h3","index":"3.4","title":"Data Preprocessing"}},{"type":"p","value":[{"type":"text","value":"Data preprocessing is a crucial step in any machine learning pipeline. Namely, in the context of k-means, how should you preprocess your dataset’s features such that Euclidean distance accurately captures the similarity between two points? Should you standardize all your features such that distance means the same thing in all directions?"}]},{"type":"p","value":[{"type":"PointerSvg","value":""},{"type":"text","value":"Try scaling and translating the distribution of either feature by dragging it. Is there a significant difference in the clusters formed?"}]},{"type":"PlaygroundControls"},{"type":"p","value":[{"type":"text","value":"In addition to standardization or normalization, there are other <a href=https://developers.google.com/machine-learning/clustering/prepare-data>preprocessing methods</a> worth exploring such as log transforms or creating quantiles."}]}]},{"type":"step","value":[{"type":"SectionTitle","props":{"tag":"h3","index":"3.5","title":"Influence of Outliers"}},{"type":"p","value":[{"type":"text","value":"K-means is sensitive to outliers—data points that differ significantly from other observations. A stray data point could drag a prototype out of its proper position, and form its own cluster instead of being ignored."}]},{"type":"p","value":[{"type":"PointerSvg","value":""},{"type":"text","value":"Add a data point that is far away from the current distribution of data points, and observe what happens. The following interactions are possible:"}]},{"type":"InteractionList"},{"type":"PlaygroundControls"}]}]}